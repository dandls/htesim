\documentclass[nojss]{jss}

%% packages
\usepackage{amstext}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{thumbpdf}
\usepackage{rotating}
%% need no \usepackage{Sweave}

%% additional commands
\newcommand{\squote}[1]{`{#1}'}
\newcommand{\dquote}[1]{``{#1}''}
\newcommand{\fct}[1]{\texttt{#1()}}
\newcommand{\class}[1]{\squote{\texttt{#1}}}

%% for internal use
\newcommand{\fixme}[1]{\emph{\marginpar{FIXME} (#1)}}
\newcommand{\readme}[1]{\emph{\marginpar{README} (#1)}}

\hyphenation{Qua-dra-tic}

\title{\pkg{htesim}: A Simulation Study Framework for Heterogeneous Treatment Effect Estimation}
\Plaintitle{htesim: A Simulation Study Framework for Heterogeneous Treatment Effect Estimation}

\author{Susanne Dandl\\Ludwig-Maximilians-Universit\"at M\"unchen
\And Torsten Hothorn\\University of Z\"urich}
\Plainauthor{Susanne Dandl, Torsten Hothorn}

\Abstract{
The \pkg{htestim} package provides an extensive simulation study framework to
evaluate methods to estimate heterogeneous treatment effects.
}
\Keywords{heterogeneous treatment effect estimation, simulation study, randomized and observational data}

\Address{
Susanne Dandl \\
Department of Statistics \\
Chair of Statistical Learning and Data Science \\
LMU Munich\\
E-mail: \email{Susanne.Dandl@stat.uni-muenchen.de} \\
URL: \url{https://www.slds.stat.uni-muenchen.de/people/dandl/} \\

Torsten Hothorn\\
Institut f\"ur Epidemiologie, Biostatistik und Pr\"avention \\
Universit\"at Z\"urich \\
Hirschengraben 84\\
CH-8001 Z\"urich, Switzerland \\
E-mail: \email{Torsten.Hothorn@R-project.org}\\
URL: \url{http://user.math.uzh.ch/hothorn/}
}


\begin{document}

<<include=FALSE>>=
library("knitr")
opts_chunk$set(
  tidy=FALSE, eval = TRUE
)
@

<<setup, echo = FALSE, results='hide'>>=
suppressWarnings(RNGversion("3.5.2"))
options(width = 70)
@



\section{Overview} \label{sec:overview}
Estimation of heterogeneous treatment effects from randomized or observational data
received a lot of attention over the last years in medical, social and economical sciences.
Methods to identify subgroups with homogeneous
treatment effects or to derive personalized treatment effects are various and
range from deploying classical lasso regression (TODO), over recursive partitioning (TODO) to
deep neural networks (TODO).
Since these methods do not perform equally good in different settings (e.g. observational
vs. randomized data, low vs. high dimensional data, constant vs. complex treatment effects, etc.),
extensive simulation studies are necessary to compare these methods empirically.
The \pkg{htesim} \proglang{R} package provides a framework to easily setup simulation studies.
that differ in
\begin{itemize}
  \item treatment, prognostic or propensity score functions
  \item distribution functions of outcomes
  \item distribution functions of covariates
  \item number of observations/samples
  \item number of explanatory variables
\end{itemize}
With this toolkit, it is easy to define various data generating processes (\code{dgp}) and to sample
study observations with known ground truth treatment effects from them (\code{simulate}).
Users could define own treatment, prognostic and propensity score functions,
or they make use of multiple predefined functions (\code{tF, mF, pF}).
These predefined functions are based on TODO and TODO.
They were also employed by xxxx to compare multiple forest-based methods.
We stored the study setting and the performance of these methods (\code{studyres})
so that others could reproduce this study and compare the performance of their methods with them.

\proglang{R} offers a few packages that are tailored to causal treatment effect estimation. Many of them are generate data based on
structural equation models and not parametric distribution functions like \pkg{htesim} (\pkg{simcausal} package (TODO), \pkg{lavaan} (TODO) and \pkg{simsem}).
Other packages only cover special cases like generalized multistate models or survival models
(\pkg{survsim}, \pkg{gems}).

The package \pkg{causalToolbox} provides the most similar functionalities to
\pkg{htesim}. It mainly focuses on estimation methods, but also includes a functions
to conduct experiments. It offers predefined underlying effect functions but
no flexibility to use own functions like \pkg{htesim}. Furthermore, it only covers normal-distributed outcomes and no other distributions.

Overall the distinguishing features of \pkg{htesim} is that
\begin{enumerate}
   \item[(1)] simulation data are based on parameteric distribution functions
   that cover simple scenarios like normal-distributed outcomes or covariates but
  also more complex scenarios like weibull-distributed outcomes to imitate survival data.
  \item[(2)] it gives the user the flexibility to define own effect functions.
  \item[(3)] it provides the necessary functionality to replicate previous studies.
\end{enumerate}

In the upcoming section, the most important functions of \pkg{htesim} are presented
followed by some examples on how to employ own effect functions and how to
replicate the study framework of xxxx.

\section{Functionalities} \label{sec:functions}

\subsection{Overview} \label{subsec:overview}

dgp --> simulate --> predict

Example:
<<setup2 >>=
library("htesim")
library("checkmate")
set.seed(8008)
@

\subsection{Define a Data Generating Process} \label{subsec:dgp}

\begin{itemize}
\item Explain dgp
\item Predefined functions (settings Athey and others)
\end{itemize}

\subsection{Simulate from Data Generating Process} \label{subsec:simulate}

\subsection{Compute Ground Truth for New Data} \label{subsec:predict}

\section{Examples} \label{sec:examples}

\subsection{Use your own Functions} \label{subsec:ownfunc}
It is also possible to specify and simulate from your own treatment effect, prognostic effect
and treatment propensity functions.
In Section~\ref{sec:functions} we saw that \code{simulate} generates a dataset of
covariates by drawing random numbers from a uniform distribution or normal distribution.
These variables are labelled as "X1", "X2", and so on.
This dataset is the input for a new function.
As an example, we want our treatment effect function to have the form
$$ \tau(\mathbf{x}) = (x_1 + x_2)^{\nu}/4 $$
with $\nu \ge 0$, our prognostic effect function should be
$$ \mu(\mathbf{x}) = exp(x_1 + 0.3 x_2 + 0.4 x_3)$$.
The propensity score should be $0.5$ for all observations, meaning that
we conduct a randomized trial.
We can our functions up using
<<owntE >>=
tE_nu_x1_x2 <- function(x, nu = 1) {
  return(((x[, "X1"] + x[, "X2"])^nu)/4)
}

mE_exp_x1_x3 <- function(x) {
  return(exp(x[, "X1"] + 0.3 * x[, "X2"] + 0.4 * x[, "X3"]))
}
@
To receive simulation study data, for example, of size $n = 500$, we pass these functions to \code{dgp} and \code{simulate} from
it.

<<simowntE >>=
dgpown <- dgp(p = 0.5, m = mE_exp_x1_x3, t = tE_nu_x1_x2)
simown <- simulate(dgpown, nsim = 500)
head(simown)
@

\subsection{Reproduce Study of xxxx} \label{subsec:dandl}
To compare the results of other methods to the methods investigated in
xxxx, the study setting and the corresponding results of the forest approaches are
available in this package.

<<loadres>>=
load(file = "../data/studyres.rda")
head(studyres)
@

The column \code{setup} corresponds to the unique data generating process defined  by \code{p}, \code{m}, \code{t}, \code{sd}, \code{pi}, \code{model}, \code{xmodel}, \code{nsim} and \code{dim}.
Each data generating process war repeated 100 times. \code{repl} stores the rerun ids.
\code{p}, \code{m}, \code{t}, \code{sd}, \code{pi} \code{pi}, \code{model} and \code{xmodel}
are arguments for \code{dgp}, while \code{nsim}, \code{dim}, \code{nsimtest} and \code{seed}
should be inserted into \code{simulate.dgp}.

The information in these columns are enough to reproduce the data generating process.
For illustration, we will generate a study dataset derived from the first
row of \code{res}.

<<row1>>=
study1 <- studyres[1,]
@

Since the column names of \code{study1} matches the arguements of \code{dgp} and \code{simulate},
we can use \code{do.call} to generate the simulated data set.
\code{seed} helps us to receive the exact and not approximate study data set of xxx.

<<getdatar>>=
dgp1 <- do.call(dgp, as.list(study1)[c("p", "m", "t", "sd", "pi", "model", "xmodel")])
# equivalent to
# dgp1 <- dgp(p = study1$p, m = study1$m, t = study1$t, sd = study1$sd, pi = study1$pi,
#  model = study1$model, xmodel = study1$xmodel)

sim1 <- do.call(simulate, c(list(object = dgp1), as.list(study1)[c("nsim", "dim", "seed", "nsimtest")]))
# equivalent to
# sim1 <- simulate(dgp1, nsim = study1$nsim, dim = study1$dim, seed = study1$seed, nsimtest = study1$nsimtest)

head(sim1)
@

The test data was attached to \code{sim1} as an attribute.

% <<gettest>>=
% test <- attr(sim1, "testxdf")
% head(test)
% @
%
% We receive the true effects using for the test data using
% <<predtest>>=
% tau <- predict(dgp1, newdata = test)
% head(tau)
% @
%
% xxx were only interested in the treatment effect estimation.
% They compared different forest-based methods to estimate these effects.
% Performance was assessed by mean squared error
% $$\frac{1}{1000} \sum_{i = 1}^{1000}(\tau(\mathbf{x}_i) - \hat{\tau}(\mathbf{x}_i))$$
% evaluated on the test sample.
% The results for each method are save in columns 14 to 23.
%
% <<algonams>>=
% # Get method names
% algos <- names(studyres)[14:23]
% @
%
% To visualize the results of Dandl et al. (2021) with \pkg{lattice}, we have
% to reshape the data from a wide to long format.
%
% <<reshape>>=
% res <- reshape(studyres, direction = "long", varying = algos,
%   idvar = c("p", "m", "t", "sd", "pi", "model", "xmodel",
%     "nsim", "dim", "nsimtest", "seed"), v.names = "value", timevar = "algorithm")
% res$algorithm <- algos[res$algorithm]
% head(res)
%
% @
% Each row corresponds now to the results of \textit{one} method for one study setting
% (unique data generating process, number of training observations and dimensions)
% and replication.
% Each row of our plot should corrspond to one data generating process (i.e., \code{setup}) and
% each row to one unique combination of \code{nsim} and \code{dim}.
%
% <<latticeopts>>=
% library("lattice")
% library("latticeExtra")
% trellis.par.set(list(plot.symbol = list(col="black",pch=18, cex=0.75),
%   box.rectangle = list(col=1),
%   box.umbrella = list(lty=1, col=1),
%   strip.background = list(col = "white")))
% ltheme <- canonical.theme(color = FALSE)     ## in-built B&W theme
% ltheme$strip.background$col <- "transparent" ## change strip bg
% lattice.options(default.theme = ltheme)
% @
%
% <<adapt>>=
% cols <- c("m4y" = "snow", "m4y-honest" = "snow4",
%   "hybrid" = "tan",  "hybrid-honest" = "tan4",
%   "equalized" = "darkorchid1", "equalized-honest" = "darkorchid4",
%   "cf" = "deepskyblue", "cf-honest" = "deepskyblue4",
%   "m4ycf" = "darkolivegreen1", "m4ycf-honest" = "darkolivegreen4")
%
% cols <- cols[-grep("honest", names(cols))]
%
% methodnams <- c("m4y",  "hybrid", "equalized",  "cf", "m4ycf")
% @
%
% <<settings>>=
%
% sc <- function(which.given, ..., factor.levels) {
%   if (which.given == 2) {
%     strip.default(which.given = which.given, ..., factor.levels)
%   } else {
%     strip.default(which.given = 1, ...,
%       factor.levels)
%   }
% }
%
% mypanel <- function(x, y, groups, subscripts, ...) {
%   fill <- cols[intersect(levels(x), unique(x))]
%   panel.bwplot(x = x, y = y, fill = fill, ...)
%   tapply(1:length(y), groups[subscripts], function(i) {
%     xi <- 1:nlevels(x)
%     yi <- y[i][match(xi, unclass(x[i]))]
%     llines(x = xi, y = yi,
%       col = rgb(0.1, 0.1, 0.1, 0.03))
%   })
% }
%
% refactor <- function(df, method.nams) {
%   plev <- c("pF_x1", "pF_x3", "pF_x4", "pF_eta_x1_x2", "0.5", "pF_x2_x3", "pF_exp_x1_x2")
%   mlev <- c("0", "mF_x1", "mF_x3", "mF_sin_x1_x5", "mF_max_x1_x5", "mF_log_x1_x3", "mF_max2_x1_x5")
%   tlev <- c("0", "tF_exp", "tF_div", "tF_log", "1", "tF_max")
%   nlev <- c(800, 1600)
%   dlev <- c(10, 20)
%
%   df$p <- factor(df$p, labels = plev, levels = plev)
%   df$m <- factor(df$m, labels = mlev, levels = mlev)
%   df$t <- factor(df$t, labels = tlev, levels = tlev)
%   df$nsim <- factor(df$nsim, labels = nlev, levels = nlev)
%   levels(df$nsim) <- paste("N = ", levels(df$nsim))
%   df$dim <- factor(df$dim, labels = dlev, levels = dlev)
%   levels(df$dim) <- paste("P = ", levels(df$dim))
%
%   df$i <- with(df, interaction(p, m, t))[, drop = TRUE]
%   df$i <- factor(df$i)
%   df$i <- droplevels(df$i)
%   df$nd <- with(df, interaction(dim, nsim, sep = ", "))
%   lev <- levels(df$nd)
%   levels(df$nd) <- sapply(strsplit(lev, ", "), function(x) paste(x[2], ", ", x[1]))
%   df$setup <- factor(df$setup)
%
%   df <- df[, c("algorithm", "setup", "repl", "i", "nd", "pi", "seed", "value")]
%
%   df$algorithm <- factor(as.character(df$algorithm), levels = method.nams,
%     labels = gsub("honest", "-honest", method.nams))
%   df <- df[!is.na(df$algorithm),]
%   names(df)[names(df) == "result.res"] <- "value"
%   return(df)
% }
%
% mykey <- list(space="top", rectangles=list(col=cols),
%   text=list(names(cols)), columns = 5)
%
% ylab <- expression(paste(frac(1, 1000), "",
%   sum((tau(x[i]) - hat(tau)(x[i]))^2, i == 1, 1000)))
%
% ### plots
% plot_results <- function(pltdata, sc, ylim) {
%   plt <- bwplot(value ~ algorithm | setup + nd, data = pltdata,
%     ylab = list(ylab), ylim = ylim,
%     groups = repl, panel = mypanel,
%     as.table = TRUE, strip = sc, key = mykey,
%     scales = list(relation = "same", x = list(rot = 60)))
%   useOuterStrips(plt, strip = sc)
% }
% @
%
% <<plot, fig.width=10, fig.height = 10.5, fig.width = 11>>=
% res <- refactor(res, method.nams =  methodnams)
%
% ### w/o overlap: W
% normalA <- subset(res, pi == 0)
% ### w overlap: W - .5
% normalB <- subset(res, pi == 0.5)
%
% plot_results(normalA[normalA$setup %in% 1:16,], sc = sc)
% plot_results(normalB[normalA$setup %in% 1:16,], sc = sc)
% plot_results(normalA[normalA$setup %in% 17:20,], sc = sc)
% plot_results(normalB[normalA$setup %in% 17:20,], sc = sc)
% @


\section{Summary} \label{summary}
This vignette (\code{"htesim"}) introduces the package \pkg{htesim} that
provides...


\bibliography{literature}

\end{document}
